{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "hyperParams = {\n",
    "    'batchSize': 128,\n",
    "    'randomState': 194,\n",
    "    'validRatio': 0.33,\n",
    "    'epochs': int(1e6),\n",
    "    'device': torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'),\n",
    "    'numWorker': 10,\n",
    "    'accumIter': 2,\n",
    "    'verboseStep': 1,\n",
    "    'minLr': 1e-4,\n",
    "    'maxLr': 1e-3,\n",
    "    'eps': 1e-8,\n",
    "    'weightDecay': 1e-3,\n",
    "    'modelDim': 32,\n",
    "    'modelDepth': 5,\n",
    "    'modelHead': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CICIDS2017(Dataset):\n",
    "    def __init__(self, data, label) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        target = self.labels[index]\n",
    "        query = self.data[index]\n",
    "        return query, target\n",
    "\n",
    "def initDataloader(dataPath, labelPath):\n",
    "     data = np.load(dataPath)\n",
    "     labels = np.load(labelPath)\n",
    "     xTrain, xTest, yTrain, yTest = train_test_split(data, labels, test_size=hyperParams['validRatio'], random_state=hyperParams['randomState'])\n",
    "\n",
    "     trainSet = CICIDS2017(xTrain, yTrain)\n",
    "     validSet = CICIDS2017(xTest, yTest)\n",
    "\n",
    "     trainLoader = torch.utils.data.DataLoader(trainSet,\n",
    "                                               batch_size=hyperParams['batchSize'],\n",
    "                                               num_workers=hyperParams['numWorker'],\n",
    "                                               pin_memory=False,\n",
    "                                               drop_last=True,\n",
    "                                               shuffle=True)\n",
    "     \n",
    "     validLoader = torch.utils.data.DataLoader(validSet,\n",
    "                                               batch_size=hyperParams['batchSize'],\n",
    "                                               num_workers=hyperParams['numWorker'],\n",
    "                                               pin_memory=False,\n",
    "                                               drop_last=True,\n",
    "                                               shuffle=True)\n",
    "     \n",
    "     return trainLoader, validLoader, data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, datasetName, exportPath, patience=20, delta=0):\n",
    "        self.report = None\n",
    "        self.dtsName = datasetName\n",
    "        self.exportPath = exportPath\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.bestLoss = None\n",
    "        self.bestAcc = None\n",
    "        self.earlyStop = False\n",
    "\n",
    "    def __call__(self, val_loss, val_acc, model, report):\n",
    "        \n",
    "        if self.bestLoss is None:\n",
    "            self.bestLoss = val_loss\n",
    "            self.bestAcc = val_acc\n",
    "            self.report = report\n",
    "\n",
    "            torch.save({'dataset': self.dtsName,\n",
    "                        'modelStateDict': model.state_dict(),\n",
    "                        'bestLoss': self.bestLoss,\n",
    "                        'bestAcc': self.bestAcc,\n",
    "                        'report': self.report}, self.exportPath)\n",
    "\n",
    "\n",
    "        elif val_loss > self.bestLoss + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            print(f'Best loss of fold {self.fold} is: {self.bestLoss}')\n",
    "            print(f'Best accuracy of fold {self.fold} is: {self.bestAcc}')\n",
    "\n",
    "            if self.counter >= self.patience:\n",
    "                self.earlyStop = True\n",
    "                \n",
    "        else:\n",
    "            self.bestLoss = val_loss\n",
    "            self.bestAcc = val_acc\n",
    "            self.report = report\n",
    "            self.counter = 0\n",
    "\n",
    "            torch.save({'dataset': self.dtsName,\n",
    "                        'modelStateDict': model.state_dict(),\n",
    "                        'bestLoss': self.bestLoss,\n",
    "                        'bestAcc': self.bestAcc,\n",
    "                        'report': self.report}, self.exportPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from RITDS.util.masks import get_mask\n",
    "\n",
    "\n",
    "def seedEverything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class ClearGPUMem:\n",
    "    def __enter__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def trainModel(epoch, model, lossFunc, optimizer, trainLoader, scheduler=None, schdBatchUpdate=False):\n",
    "        model.train()\n",
    "        runningLoss = None\n",
    "\n",
    "        pbar = tqdm(enumerate(trainLoader), total=len(trainLoader))\n",
    "        for step, (query, target) in pbar:\n",
    "            query = query.to(hyperParams['device']).float()\n",
    "            target = target.to(hyperParams['device']).long()\n",
    "\n",
    "            scaler = GradScaler()\n",
    "            with autocast():\n",
    "                mask = get_mask(hyperParams['batchSize'], hyperParams['modelHead'], query.shape[1]).to(hyperParams['device'])\n",
    "                predict = model(query, mask)\n",
    "                \n",
    "                loss = lossFunc(predict, target)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if runningLoss is None:\n",
    "                    runningLoss = loss.item()\n",
    "                else:\n",
    "                    runningLoss = runningLoss * .99 + loss.item() * .01\n",
    "\n",
    "                if ((step + 1) %  hyperParams['accumIter'] == 0) or ((step + 1) == len(trainLoader)):\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad() \n",
    "                    \n",
    "                    if scheduler is not None and schdBatchUpdate:\n",
    "                        scheduler.step()\n",
    "\n",
    "                if ((step + 1) % hyperParams['verboseStep'] == 0) or ((step + 1) == len(trainLoader)):\n",
    "                    description = f'Epoch {epoch} loss: {runningLoss:.4f}'\n",
    "                    pbar.set_description(description)\n",
    "                  \n",
    "        if scheduler is not None and not schdBatchUpdate:\n",
    "           scheduler.step()\n",
    "\n",
    "def evalModel(epoch, model, lossFn, validLoader, earlyStopping=None, labelmap=None, scheduler=None, schd_loss_update=False):      \n",
    "        model.eval()\n",
    "\n",
    "        lossSum = 0\n",
    "        sampleNum = 0\n",
    "        predictAll = []\n",
    "        targetAll = []\n",
    "        \n",
    "        pbar = tqdm(enumerate(validLoader), total=len(validLoader))\n",
    "        for step, (query, target) in pbar:\n",
    "            query = query.to(hyperParams['device']).float()\n",
    "            target = target.to(hyperParams['device']).long()\n",
    "            \n",
    "            predict = model(query)\n",
    "\n",
    "            predictAll += [torch.argmax(predict, 1).detach().cpu().numpy()]\n",
    "            targetAll += [target.detach().cpu().numpy()]\n",
    "            \n",
    "            loss = lossFn(predict, target)\n",
    "            \n",
    "            lossSum += loss.item() * target.shape[0]\n",
    "            sampleNum += target.shape[0]\n",
    "\n",
    "            if ((step + 1) % hyperParams['verboseStep'] == 0) or ((step + 1) == len(validLoader)):\n",
    "                description = f'Epoch {epoch} loss: {lossSum/sampleNum:.4f}'\n",
    "                pbar.set_description(description)\n",
    "        \n",
    "        predictAll = np.concatenate(predictAll)\n",
    "        targetAll = np.concatenate(targetAll)\n",
    "\n",
    "        report = classification_report(targetAll, predictAll, target_names=labelmap, digits=4)\n",
    "        Loss = lossSum/sampleNum\n",
    "        Acc = (predictAll==targetAll).mean()\n",
    "\n",
    "        print(\"---Classification Report---\")\n",
    "        print(report)\n",
    "\n",
    "        print(f'Validating loss: {Loss}')\n",
    "        print(f'Validating accuracy: {Acc}')\n",
    "\n",
    "        if earlyStopping != None:\n",
    "            earlyStopping(Loss, Acc, model, report)\n",
    "          \n",
    "        if scheduler is not None:\n",
    "            if schd_loss_update:\n",
    "                scheduler.step(Loss)\n",
    "            else:\n",
    "                scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from RITDS.transformer import RTIDS_Transformer\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn \n",
    "\n",
    "seedEverything(hyperParams['randomState'])\n",
    "trainLoader, validLoader, numFeatures = initDataloader(dataPath='./preprocessedData/data.npy', labelPath='./preprocessedData/label.npy')\n",
    "labelMappingJson = json.load(open('./preprocessedData/labelMapping.json'))\n",
    "labelMapping = [label for label in labelMappingJson.values()]\n",
    "numClass = len(labelMapping)\n",
    "\n",
    "model = RTIDS_Transformer(numClass=numClass, dim=hyperParams['modelDim'], depth=hyperParams['modelDepth'], \n",
    "                          heads=hyperParams['modelHead'], maskSize=numFeatures).to(hyperParams['device'])\n",
    "\n",
    "with ClearGPUMem():\n",
    "    optimizer = AdamW(model.parameters(), lr=hyperParams['maxLr'], eps=hyperParams['eps'], weight_decay=hyperParams['weightDecay'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=hyperParams['minLr'])\n",
    "\n",
    "    trainLossFunc = nn.CrossEntropyLoss().to(hyperParams['device'])\n",
    "    validLossFunc = nn.CrossEntropyLoss().to(hyperParams['device'])\n",
    "                        \n",
    "    earlyStopping = EarlyStopping(datasetName='CICIDS2017', exportPath=\"./modelCheckpoint/RITDS.checkpoint\", patience=10)\n",
    "    \n",
    "    for epoch in range(hyperParams['epochs']):\n",
    "        print('=================================================')\n",
    "        print(f'\\n[ TRAINING EPOCH {epoch} ]')\n",
    "        TrainTime = trainModel(epoch, model, trainLossFunc, optimizer, \n",
    "                               trainLoader, scheduler)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            print('\\n[ EVALUATING VALIDATION ACCURACY ]')\n",
    "            evalModel(epoch, model, validLossFunc, validLoader, \n",
    "                      earlyStopping, labelMapping, scheduler)\n",
    "            \n",
    "            if earlyStopping.earlyStop:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RTIDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
